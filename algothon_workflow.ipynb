{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cryptpandas as crp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium_stealth import stealth\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Slack Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLACK_BOT_TOKEN = \"BOT_TOKEN\" \n",
    "CHANNEL_NAME = \"#the-challenge\" \n",
    "AUTHORIZED_USER_ID = \"ID\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Slack channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WebClient(token=SLACK_BOT_TOKEN)\n",
    "\n",
    "def get_channel_id(channel_name):\n",
    "    try:\n",
    "        response = client.conversations_list()\n",
    "        for channel in response[\"channels\"]:\n",
    "            if channel[\"name\"] == channel_name.lstrip(\"#\"):\n",
    "                return channel[\"id\"]\n",
    "    except SlackApiError as e:\n",
    "        print(f\"Error fetching channels: {e.response['error']}\")\n",
    "    return None\n",
    "\n",
    "def extract_id_and_password(message):\n",
    "    match = re.search(r\"release_(\\d+)\\.crypt.*?passcode is '(.*?)'\", message)\n",
    "    if match:\n",
    "        file_id = match.group(1)\n",
    "        password = match.group(2)\n",
    "        return file_id, password\n",
    "    return None\n",
    "\n",
    "def monitor_channel(channel_id, latest_timestamp):\n",
    "    datasets = []\n",
    "    try:\n",
    "        response = client.conversations_history(channel=channel_id, oldest=latest_timestamp, limit=100)\n",
    "        for message in reversed(response[\"messages\"]):\n",
    "            if message.get(\"user\") == AUTHORIZED_USER_ID and \"Data has just been released\" in message[\"text\"]:\n",
    "                file_id, password = extract_id_and_password(message[\"text\"])\n",
    "                if file_id and password:\n",
    "                    datasets.append((message[\"ts\"], file_id, password))\n",
    "    except SlackApiError as e:\n",
    "        print(f\"Error reading channel history: {e.response['error']}\")\n",
    "    return datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_id, password):\n",
    "    try:\n",
    "        file_name = f\"release_{file_id}.crypt\"\n",
    "        file_path = f\"./{file_name}\"\n",
    "        print(f\"Loading dataset: {file_name}\")\n",
    "        X = crp.read_encrypted(path=file_path, password=password)\n",
    "        print(f\"Dataset loaded successfully. Shape: {X.shape}\")\n",
    "        return X\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(series, lag=1):\n",
    "    lagged_data = pd.concat([series.shift(i) for i in range(lag, 0, -1)], axis=1)\n",
    "    lagged_data.columns = [f'lag_{i}' for i in range(lag, 0, -1)]\n",
    "    return lagged_data\n",
    "\n",
    "def compute_predictions(X):\n",
    "    df = X.copy()\n",
    "    lag = 5  \n",
    "    predictions = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        lagged_features = create_lagged_features(df[col], lag=lag)\n",
    "        lagged_features['target'] = df[col]\n",
    "        lagged_features.dropna(inplace=True)\n",
    "\n",
    "        X_full = lagged_features.drop('target', axis=1)\n",
    "        y_full = lagged_features['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_full, y_full,\n",
    "            test_size=0.2,\n",
    "            shuffle=False \n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "        param_grid = {\n",
    "            'alpha': [0.01, 0.1, 1.0, 10, 100]\n",
    "        }\n",
    "        ridge_search = GridSearchCV(\n",
    "            estimator=Ridge(),\n",
    "            param_grid=param_grid,\n",
    "            cv=TimeSeriesSplit(n_splits=3),  # time-series cross-validation\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        ridge_search.fit(X_train_scaled, y_train)\n",
    "        best_alpha = ridge_search.best_params_['alpha']\n",
    "\n",
    "        model = Ridge(alpha=best_alpha)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        next_input = X_test_scaled[-1:].copy()\n",
    "        next_prediction = model.predict(next_input)\n",
    "        predictions[col] = next_prediction[0]\n",
    "\n",
    "    def normalize_positions(pos_dict):\n",
    "        pos = pd.Series(pos_dict).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        abs_sum = pos.abs().sum()\n",
    "        if abs_sum > 0:\n",
    "            pos = pos / abs_sum\n",
    "        pos = pos.clip(-0.1, 0.1)\n",
    "        if pos.abs().sum() > 0:\n",
    "            pos = pos / pos.abs().sum()\n",
    "        return pos\n",
    "\n",
    "    def get_submission_dict(pos_dict, team_name=\"Jean Trading 69\", passcode=\"JeanForTheWin\"):\n",
    "        positions = normalize_positions(pos_dict)\n",
    "        return {**positions.to_dict(), \"team_name\": team_name, \"passcode\": passcode}\n",
    "\n",
    "    return get_submission_dict(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit automatically in Google form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIN_URL = \"https://accounts.google.com/signin\"\n",
    "GOOGLE_FORM_URL = \"https://docs.google.com/forms/d/e/1FAIpQLSeUYMkI5ce18RL2aF5C8I7mPxF7haH23VEVz7PQrvz0Do0NrQ/viewform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "ua = UserAgent()\n",
    "options.add_argument(f'user-agent={ua.random}')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", webgl_vendor=\"Intel Inc.\", renderer=\"Intel Iris OpenGL Engine\", fix_hairline=True)\n",
    "\n",
    "def submit_to_google_form(submission_dict):\n",
    "    driver.get(GOOGLE_FORM_URL)\n",
    "    textarea = driver.find_element(By.XPATH, \"/html/body/div/div[2]/form/div[2]/div/div[2]/div[2]/div/div/div[2]/div/div[1]/div[2]/textarea\")\n",
    "    textarea.clear()\n",
    "    textarea.send_keys(str(submission_dict).replace(\"'\", '\"'))\n",
    "    print(\"Form filled successfully. Please review and submit manually.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Initializing monitoring...\")\n",
    "    channel_id = get_channel_id(CHANNEL_NAME)\n",
    "    latest_timestamp = \"0\"\n",
    "    processed_datasets = set()\n",
    "\n",
    "    while True:\n",
    "        datasets = monitor_channel(channel_id, latest_timestamp)\n",
    "        for ts, file_id, password in datasets:\n",
    "            if file_id not in processed_datasets:\n",
    "                X = load_dataset(file_id, password)\n",
    "                if X is not None:\n",
    "                    submission = compute_predictions(X)\n",
    "                    submit_to_google_form(submission)\n",
    "                processed_datasets.add(file_id)\n",
    "                latest_timestamp = ts\n",
    "        time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
